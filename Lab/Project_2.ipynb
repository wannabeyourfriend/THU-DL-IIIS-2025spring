{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60d4eca-13d2-46e3-8f62-d266ab11072a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取唐诗数据集\n",
    "#你可以从以下来源获取大量的唐诗数据集：\n",
    "#网络爬虫：可以编写爬虫从网上的唐诗数据库中爬取数据，如中国古诗词网。https://www.gushiwen.cn/\n",
    "#现成数据集：可以在一些数据平台上找到现成的唐诗数据集，如Kaggle或GitHub。\n",
    "#例如，在GitHub上可以找到一些唐诗数据集：\n",
    "#chinese-poetry: 这个项目包含了大量的中国古诗，包括唐诗、宋词等。https://github.com/chinese-poetry/chinese-poetry\n",
    "#以下是一个简单的脚本，展示如何从 chinese-poetry 项目中提取唐诗并保存为 tang_poems.txt 文件。\n",
    "\"\"\"\n",
    "静夜思 李白\n",
    "床前明月光，疑是地上霜。\n",
    "举头望明月，低头思故乡。\n",
    "\n",
    "春晓 孟浩然\n",
    "春眠不觉晓，处处闻啼鸟。\n",
    "夜来风雨声，花落知多少。\n",
    "\n",
    "登鹳雀楼 王之涣\n",
    "白日依山尽，黄河入海流。\n",
    "欲穷千里目，更上一层楼。\n",
    "\"\"\"\n",
    "import json\n",
    "# 下载 chinese-poetry 项目中的唐诗数据集\n",
    "#!git clone https://github.com/chinese-poetry/chinese-poetry.git\n",
    "#\"C:\\Users\\drhu0\\chinese-poetry\"\n",
    "# 读取唐诗数据\n",
    "#poems_file_path = 'C:\\\\Users\\\\drhu0\\\\data\\\\poetry\\\\poet.tang.7000.json'\n",
    "poems_file_path = 'C:\\\\Users\\\\drhu0\\\\data\\\\poetry\\\\poet.tang.1000.json'\n",
    "with open(poems_file_path, 'r', encoding='utf-8') as f:\n",
    "    poems_data = json.load(f)\n",
    "\n",
    "# 保存为 tang_poems.txt 文件\n",
    "with open('tang_poems_1000.txt', 'w', encoding='utf-8') as f:\n",
    "    for poem in poems_data:\n",
    "        title = poem['title']\n",
    "        author = poem['author']\n",
    "        content = '\\n'.join(poem['paragraphs'])\n",
    "        f.write(f\"{title} {author}\\n{content}\\n\\n\")\n",
    "print(\"唐诗数据集已保存为 tang_poems.txt 文件\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79a1551-c716-4310-8e45-f3ee2ae21e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import opencc\n",
    "\n",
    "# 创建一个转换器\n",
    "converter = opencc.OpenCC('t2s')\n",
    "\n",
    "#https://www.heywhale.com/home/global?search=%E5%85%A8%E5%94%90%E8%AF%97%26%E5%85%A8%E5%AE%8B%E8%AF%97%E6%95%B0%E6%8D%AE%E9%9B%86\n",
    "#这个数据集包含5.5万首唐诗和26万首宋诗，以及相关的作者信息，存储为JSON文件。每个JSON文件包含1000篇诗，数据以繁体字存储，\n",
    "#但您可能需要将其转换为简体字以适应transformers库的要求。\n",
    "\n",
    "# 假设您的JSON文件存放在'path_to_json_files'目录下\n",
    "#json_files_dir = 'C:\\\\Users\\\\drhu0\\\\data\\\\poetry\\\\poet.tang.7000.json'\n",
    "\n",
    "json_files_dir = 'C:\\\\Users\\\\drhu0\\\\data\\\\poetry\\\\'\n",
    "# 保存为 poems.txt 文件\n",
    "with open('poems.txt', 'a', encoding='utf-8') as f:\n",
    "    for filename in os.listdir(json_files_dir):\n",
    "        if filename.endswith('.json'):\n",
    "            file_path = os.path.join(json_files_dir, filename)\n",
    "            # 读取JSON文件\n",
    "            print(file_path)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                data = json.load(file)\n",
    "    \n",
    "            # 对数据集中的文本进行转换\n",
    "            for poem in data:\n",
    "                if 'paragraphs' in poem:\n",
    "                    poem['paragraphs'] = [converter.convert(text) for text in poem['paragraphs']]\n",
    "                    content = '\\n'.join(poem['paragraphs'])\n",
    "                    poem['title'] = converter.convert(poem['title'])\n",
    "                    title = poem['title']\n",
    "                    if 'author' in poem:\n",
    "                        poem['author'] = converter.convert(poem['author'])\n",
    "                        author = poem['author']\n",
    "\n",
    "                    f.write(f\"{title} {author}\\n{content}\\n\\n\")\n",
    "            \n",
    "            \n",
    "            # 将转换后的数据写回文件\n",
    "            with open(file_path, 'w', encoding='utf-8') as file:\n",
    "                json.dump(data, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddb88e6-1564-4e82-bbdf-2ba66a36b471",
   "metadata": {},
   "outputs": [],
   "source": [
    "#project 2: 使用预训练框架写作中文诗歌\n",
    "\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dropout, Dense, Flatten, Bidirectional, Embedding, GRU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "puncs = [']', '[', '（', '）', '{', '}', '：', '《', '》']\n",
    "\n",
    "def preprocess_file(Config):\n",
    "    # 语料文本内容\n",
    "    files_content = ''\n",
    "    with open(Config.poetry_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            # 每行的末尾加上\"]\"符号代表一首诗结束\n",
    "            for char in puncs:\n",
    "                line = line.replace(char, \"\")\n",
    "            files_content += line.strip() + \"]\"\n",
    "\n",
    "    words = sorted(list(files_content))\n",
    "    words.remove(']')\n",
    "    counted_words = {}\n",
    "    for word in words:\n",
    "        if word in counted_words:\n",
    "            counted_words[word] += 1\n",
    "        else:\n",
    "            counted_words[word] = 1\n",
    "\n",
    "    # 去掉低频的字\n",
    "    erase = []\n",
    "    for key in counted_words:\n",
    "        if counted_words[key] <= 2:\n",
    "            erase.append(key)\n",
    "    for key in erase:\n",
    "        del counted_words[key]\n",
    "    del counted_words[']']\n",
    "    wordPairs = sorted(counted_words.items(), key=lambda x: -x[1])\n",
    "\n",
    "    words, _ = zip(*wordPairs)\n",
    "    # word到id的映射\n",
    "    word2num = dict((c, i + 1) for i, c in enumerate(words))\n",
    "    num2word = dict((i, c) for i, c in enumerate(words))\n",
    "    word2numF = lambda x: word2num.get(x, 0)\n",
    "    return word2numF, num2word, words, files_content\n",
    "\n",
    "class PoetryModel(object):\n",
    "    def __init__(self, config):\n",
    "        self.model = None\n",
    "        self.do_train = True\n",
    "        self.loaded_model = False\n",
    "        self.config = config\n",
    "\n",
    "        # 文件预处理\n",
    "        self.word2numF, self.num2word, self.words, self.files_content = preprocess_file(self.config)\n",
    "\n",
    "        # 如果模型文件存在则直接加载模型，否则开始训练\n",
    "        if os.path.exists(self.config.weight_file):\n",
    "            self.model = load_model(self.config.weight_file)\n",
    "            self.model.summary()\n",
    "        else:\n",
    "            self.train()\n",
    "        self.do_train = False\n",
    "        self.loaded_model = True\n",
    "\n",
    "    def build_model(self):\n",
    "        '''建立模型'''\n",
    "\n",
    "        # 输入的dimension\n",
    "        input_tensor = Input(shape=(self.config.max_len,))\n",
    "        embedd = Embedding(len(self.num2word) + 2, 300, input_length=self.config.max_len)(input_tensor)\n",
    "        lstm = Bidirectional(GRU(128, return_sequences=True))(embedd)\n",
    "        # dropout = Dropout(0.6)(lstm)\n",
    "        # lstm = LSTM(256)(dropout)\n",
    "        # dropout = Dropout(0.6)(lstm)\n",
    "        flatten = Flatten()(lstm)\n",
    "        dense = Dense(len(self.words), activation='softmax')(flatten)\n",
    "        self.model = Model(inputs=input_tensor, outputs=dense)\n",
    "        optimizer = Adam(learning_rate=self.config.learning_rate)\n",
    "        self.model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    def sample(self, preds, temperature=1.0):\n",
    "        '''\n",
    "        当temperature=1.0时，模型输出正常\n",
    "        当temperature=0.5时，模型输出比较open\n",
    "        当temperature=1.5时，模型输出比较保守\n",
    "        在训练的过程中可以看到temperature不同，结果也不同\n",
    "        '''\n",
    "        preds = np.asarray(preds).astype('float64')\n",
    "        preds = np.log(preds) / temperature\n",
    "        exp_preds = np.exp(preds)\n",
    "        preds = exp_preds / np.sum(exp_preds)\n",
    "        probas = np.random.multinomial(1, preds, 1)\n",
    "        return np.argmax(probas)\n",
    "\n",
    "    def generate_sample_result(self, epoch, logs):\n",
    "        '''训练过程中，每个epoch打印出当前的学习情况'''\n",
    "        # if epoch % 5 != 0:\n",
    "        #     return\n",
    "        print(\"\\n==================Epoch {}=====================\".format(epoch))\n",
    "        for diversity in [0.5, 1.0, 1.5]:\n",
    "            print(\"------------Diversity {}--------------\".format(diversity))\n",
    "            start_index = random.randint(0, len(self.files_content) - self.config.max_len - 1)\n",
    "            generated = ''\n",
    "            sentence = self.files_content[start_index: start_index + self.config.max_len]\n",
    "            generated += sentence\n",
    "            for i in range(20):\n",
    "                x_pred = np.zeros((1, self.config.max_len))\n",
    "                for t, char in enumerate(sentence[-6:]):\n",
    "                    x_pred[0, t] = self.word2numF(char)\n",
    "\n",
    "                preds = self.model.predict(x_pred, verbose=0)[0]\n",
    "                next_index = self.sample(preds, diversity)\n",
    "                next_char = self.num2word[next_index]\n",
    "\n",
    "                generated += next_char\n",
    "                sentence = sentence + next_char\n",
    "            print(sentence)\n",
    "\n",
    "    def predict(self, text):\n",
    "        '''根据给出的文字，生成诗句'''\n",
    "        if not self.loaded_model:\n",
    "            return\n",
    "        with open(self.config.poetry_file, 'r', encoding='utf-8') as f:\n",
    "            file_list = f.readlines()\n",
    "        random_line = random.choice(file_list)\n",
    "        # 如果给的text不到四个字，则随机补全\n",
    "        if not text or len(text) != 4:\n",
    "            for _ in range(4 - len(text)):\n",
    "                random_str_index = random.randrange(0, len(self.words))\n",
    "                text += self.num2word.get(random_str_index) if self.num2word.get(random_str_index) not in [',', '。',\n",
    "                                                                                                           '，'] else self.num2word.get(\n",
    "                    random_str_index + 1)\n",
    "        seed = random_line[-(self.config.max_len):-1]\n",
    "\n",
    "        res = ''\n",
    "\n",
    "        seed = 'c' + seed\n",
    "\n",
    "        for c in text:\n",
    "            seed = seed[1:] + c\n",
    "            for j in range(5):\n",
    "                x_pred = np.zeros((1, self.config.max_len))\n",
    "                for t, char in enumerate(seed):\n",
    "                    x_pred[0, t] = self.word2numF(char)\n",
    "\n",
    "                preds = self.model.predict(x_pred, verbose=0)[0]\n",
    "                next_index = self.sample(preds, 1.0)\n",
    "                next_char = self.num2word[next_index]\n",
    "                seed = seed[1:] + next_char\n",
    "            res += seed\n",
    "        return res\n",
    "\n",
    "    def data_generator(self):\n",
    "        '''生成器生成数据'''\n",
    "        i = 0\n",
    "        while 1:\n",
    "            x = self.files_content[i: i + self.config.max_len]\n",
    "            y = self.files_content[i + self.config.max_len]\n",
    "\n",
    "            puncs = [']', '[', '（', '）', '{', '}', '：', '《', '》', ':']\n",
    "            if len([i for i in puncs if i in x]) != 0:\n",
    "                i += 1\n",
    "                continue\n",
    "            if len([i for i in puncs if i in y]) != 0:\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            y_vec = np.zeros(\n",
    "                shape=(1, len(self.words)),\n",
    "                dtype=np.bool\n",
    "            )\n",
    "            y_vec[0, self.word2numF(y)] = 1.0\n",
    "\n",
    "            x_vec = np.zeros(\n",
    "                shape=(1, self.config.max_len),\n",
    "                dtype=np.int32\n",
    "            )\n",
    "\n",
    "            for t, char in enumerate(x):\n",
    "                x_vec[0, t] = self.word2numF(char)\n",
    "            yield x_vec, y_vec\n",
    "            i += 1\n",
    "\n",
    "    def train(self):\n",
    "        '''训练模型'''\n",
    "        number_of_epoch = len(self.files_content) // self.config.batch_size\n",
    "\n",
    "        if not self.model:\n",
    "            self.build_model()\n",
    "\n",
    "        self.model.summary()\n",
    "\n",
    "        self.model.fit(\n",
    "            self.data_generator(),\n",
    "            verbose=True,\n",
    "            steps_per_epoch=self.config.batch_size,\n",
    "            epochs=number_of_epoch,\n",
    "            callbacks=[\n",
    "                tf.keras.callbacks.ModelCheckpoint(self.config.weight_file, save_weights_only=False),\n",
    "                LambdaCallback(on_epoch_end=self.generate_sample_result)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "class Config(object):\n",
    "    poetry_file = 'poetry.txt'\n",
    "    weight_file = 'poetry_model.h5'\n",
    "    # 根据前六个字预测第七个字\n",
    "    max_len = 6\n",
    "    batch_size = 512\n",
    "    learning_rate = 0.001\n",
    "\n",
    "model = PoetryModel(Config)\n",
    "while 1:\n",
    "    text = input(\"text:\")\n",
    "    sentence = model.predict(text)\n",
    "    print(sentence)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2c2302-b76a-47a2-99a3-ae539481423c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#project 2: 使用循环神经网络 (RNN)\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Activation, Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# 加载数据\n",
    "def load_data(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    return text\n",
    "\n",
    "# 创建字符到索引和索引到字符的映射\n",
    "def create_mapping(text):\n",
    "    chars = sorted(list(set(text)))\n",
    "    char2idx = {c: i for i, c in enumerate(chars)}\n",
    "    idx2char = {i: c for i, c in enumerate(chars)}\n",
    "    return char2idx, idx2char\n",
    "\n",
    "# 构建输入序列和标签\n",
    "def create_sequences(text, char2idx, seq_length):\n",
    "    X, y = [], []\n",
    "    for i in range(0, len(text) - seq_length):\n",
    "        seq = text[i:i+seq_length]\n",
    "        target = text[i+seq_length]\n",
    "        X.append([char2idx[char] for char in seq])\n",
    "        y.append(char2idx[target])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# 加载数据集\n",
    "text = load_data('poems.txt')\n",
    "\n",
    "# 创建字符映射\n",
    "char2idx, idx2char = create_mapping(text)\n",
    "\n",
    "# 序列长度\n",
    "seq_length = 10\n",
    "\n",
    "# 创建序列\n",
    "X, y = create_sequences(text, char2idx, seq_length)\n",
    "\n",
    "# 构建模型\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(char2idx), 128, input_length=seq_length))\n",
    "model.add(SimpleRNN(128, return_sequences=False))\n",
    "model.add(Dense(len(char2idx)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# 编译模型\n",
    "model.compile(optimizer=Adam(lr=0.001), loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# 训练模型\n",
    "model.fit(X, y, epochs=20, batch_size=64)\n",
    "\n",
    "# 生成文本\n",
    "def generate_text(model, start_text, char2idx, idx2char, length):\n",
    "    input_text = start_text\n",
    "    generated = input_text\n",
    "    for _ in range(length):\n",
    "        x = np.array([[char2idx[char] for char in input_text]])\n",
    "        preds = model.predict(x, verbose=0)[0]\n",
    "        next_index = np.argmax(preds)\n",
    "        next_char = idx2char[next_index]\n",
    "        generated += next_char\n",
    "        input_text = input_text[1:] + next_char\n",
    "    return generated\n",
    "\n",
    "# 示例生成\n",
    "start_text = '春眠不觉晓'\n",
    "generated_text = generate_text(model, start_text, char2idx, idx2char, 100)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15841f63-96c4-48c8-8b86-b181aa23bef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project 2, 以下是一个完整的Transformer模型代码示例，使用注意力机制进行中文诗歌生成。假设你的数据文件名为 poems.txt，每行包含一首诗。\n",
    "#数据准备\n",
    "#首先，加载并预处理数据。\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 读取数据集\n",
    "def load_data(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        lines = f.read().split('\\n')\n",
    "    return [line.strip() for line in lines if len(line) > 0]\n",
    "\n",
    "# 分词和编码\n",
    "def tokenize(lang):\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "    return tensor, lang_tokenizer\n",
    "\n",
    "# 加载和预处理数据\n",
    "def load_dataset(path, num_examples=None):\n",
    "    inp_lang = load_data(path)\n",
    "    inp_tensor, inp_tokenizer = tokenize(inp_lang[:num_examples])\n",
    "    return inp_tensor, inp_tokenizer\n",
    "\n",
    "num_examples = 30000\n",
    "path_to_file = 'poems.txt'\n",
    "inp_tensor, inp_tokenizer = load_dataset(path_to_file, num_examples)\n",
    "\n",
    "# 创建数据集\n",
    "BUFFER_SIZE = len(inp_tensor)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = BUFFER_SIZE // BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_size = len(inp_tokenizer.word_index) + 1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inp_tensor, inp_tensor))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "#模型构建\n",
    "#定义Transformer模型，包括编码器、解码器和注意力机制。\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "        scaled_attention, attention_weights = self.scaled_dot_product_attention(q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
    "\n",
    "        output = self.dense(concat_attention)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "    def scaled_dot_product_attention(self, q, k, v, mask):\n",
    "        matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "\n",
    "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "        if mask is not None:\n",
    "            scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "        output = tf.matmul(attention_weights, v)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = self.point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        attn_output, _ = self.mha(x, x, x, mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "        return out2\n",
    "\n",
    "    def point_wise_feed_forward_network(self, d_model, dff):\n",
    "        return tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(dff, activation='relu'),\n",
    "            tf.keras.layers.Dense(d_model)\n",
    "        ])\n",
    "\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = self.point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)\n",
    "\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2\n",
    "\n",
    "    def point_wise_feed_forward_network(self, d_model, dff):\n",
    "        return tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(dff, activation='relu'),\n",
    "            tf.keras.layers.Dense(d_model)\n",
    "        ])\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = self.positional_encoding(input_vocab_size, self.d_model)\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        angle_rads = self.get_angles(np.arange(position)[:, np.newaxis], np.arange(d_model)[np.newaxis, :], d_model)\n",
    "\n",
    "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "    def get_angles(self, pos, i, d_model):\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "        return pos * angle_rates\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = self.positional_encoding(target_vocab_size, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
    "\n",
    "            attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
    "            attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
    "\n",
    "        return x, attention_weights\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        angle_rads = self.get_angles(np.arange(position)[:, np.newaxis], np.arange(d_model)[np.newaxis, :], d_model)\n",
    "\n",
    "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "    def get_angles(self, pos, i, d_model):\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "        return pos * angle_rates\n",
    "\n",
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask\n",
    "\n",
    "def create_masks(inp, tar):\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask\n",
    "\n",
    "# 超参数\n",
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1\n",
    "\n",
    "encoder = Encoder(num_layers, d_model, num_heads, dff, vocab_size, dropout_rate)\n",
    "decoder = Decoder(num_layers, d_model, num_heads, dff, vocab_size, dropout_rate)\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "checkpoint_path = \"./checkpoints/train\"\n",
    "ckpt = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print(\"Latest checkpoint restored!\")\n",
    "\n",
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, tar)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        tar_inp = tar[:, :-1]\n",
    "        tar_real = tar[:, 1:]\n",
    "\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            enc_output = encoder(inp, True, enc_padding_mask)\n",
    "            dec_output, _ = decoder(tar_inp, enc_output, True, combined_mask, dec_padding_mask)\n",
    "            loss = loss_function(tar_real, dec_output)\n",
    "\n",
    "        gradients = tape.gradient(loss, encoder.trainable_variables + decoder.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, encoder.trainable_variables + decoder.trainable_variables))\n",
    "\n",
    "        total_loss += loss\n",
    "\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        ckpt_manager.save()\n",
    "\n",
    "    print(f'Epoch {epoch+1} Loss {total_loss / steps_per_epoch:.4f}')\n",
    "\n",
    "def evaluate(inp_sentence):\n",
    "    start_token = [inp_tokenizer.word_index['<start>']]\n",
    "    end_token = [inp_tokenizer.word_index['<end>']]\n",
    "\n",
    "    inp_sentence = start_token + inp_tokenizer.texts_to_sequences([inp_sentence])[0] + end_token\n",
    "    encoder_input = tf.expand_dims(inp_sentence, 0)\n",
    "\n",
    "    encoder_padding_mask = create_padding_mask(encoder_input)\n",
    "    enc_output = encoder(encoder_input, False, encoder_padding_mask)\n",
    "\n",
    "    decoder_input = tf.expand_dims([inp_tokenizer.word_index['<start>']], 0)\n",
    "    output = tf.expand_dims([inp_tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    for i in range(20):\n",
    "        look_ahead_mask = create_look_ahead_mask(tf.shape(output)[1])\n",
    "        dec_target_padding_mask = create_padding_mask(output)\n",
    "        combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "        dec_output, attention_weights = decoder(decoder_input, enc_output, False, combined_mask, encoder_padding_mask)\n",
    "\n",
    "        predictions = dec_output[:, -1:, :]\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        if predicted_id == inp_tokenizer.word_index['<end>']:\n",
    "            return tf.squeeze(output, axis=0), attention_weights\n",
    "\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "        decoder_input = output\n",
    "\n",
    "    return tf.squeeze(output, axis=0), attention_weights\n",
    "\n",
    "def translate(sentence):\n",
    "    result, attention_weights = evaluate(sentence)\n",
    "\n",
    "    predicted_sentence = ' '.join([inp_tokenizer.index_word[i] for i in result.numpy() if i != 0])\n",
    "    return predicted_sentence\n",
    "\n",
    "# 示例调用\n",
    "print(translate(\"春眠不觉晓，处处闻啼鸟。\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6871df3a-cc8a-468d-a6e2-a4cfaf8e235b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b287ec-310e-46b8-8e08-9f5233650ba3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
